# GPT 概览

## 什么是 GPT？

GPT（Generative Pre-trained Transformer）是一类基于 Transformer 架构的大语言模型，通过在大规模文本语料上进行自监督预训练，学习语言的统计规律与语义表示。在预训练之后，模型能够在极少甚至无需微调的情况下，完成多种自然语言处理任务。

核心特性包括：

- **自回归生成**：根据已有上下文一步步预测下一个 token，从而生成连贯的文本。
- **上下文学习**：通过提示词在推理阶段注入任务描述或示例，实现“零样本/少样本”学习。
- **泛化能力**：同一个模型可以胜任问答、翻译、摘要、代码补全等多种任务。

## 模型版本与命名

OpenAI 相继推出了多个 GPT 系列模型：

| 发布年份 | 代表模型 | 主要特点 |
| -------- | -------- | -------- |
| 2018     | GPT      | 第一代 Transformer 语言模型，参数量 1.17 亿 |
| 2019     | GPT-2    | 参数量提升至 15 亿，引入更广泛的语料训练 |
| 2020     | GPT-3    | 参数量扩大到 1750 亿，具备强大的零样本能力 |
| 2022     | GPT-3.5  | 在对话与指令跟随上显著强化 |
| 2023+    | GPT-4 系列 | 推理能力、工具调用、多模态理解全面提升 |

> 注：不同 API 提供的模型名称（如 `gpt-4o`, `gpt-4o-mini`, `gpt-3.5-turbo`）反映了具体的能力和适用场景，请查阅官方文档获取最新列表。

## 基础术语

- **Prompt（提示词）**：输入给模型的文本，包括指令、背景、示例等，用于引导模型输出目标内容。
- **Completion（补全）**：模型对提示词的响应，也可称为生成结果。
- **Token（标记）**：文本的最小处理单元，可理解为词或子词。计费、上下文长度均以 token 为单位。
- **上下文长度**：模型一次性可处理的 token 数上限。超过限制会导致截断或报错。
- **系统提示（System Prompt）**：在对话式接口中，先于用户输入的角色设定，能够显著影响模型行为。

## 能力边界与评估

- GPT 擅长语言理解、常识推理、信息总结、结构化提取等任务。
- 对于需要实时数据、专业知识或精确数值计算的场景，应结合外部工具或知识库。
- 可以通过构建评测集、AB 测试、人工质检等方式评估 GPT 的响应质量。

继续阅读 [features.md](features.md) 了解 GPT 的具体功能模块。
